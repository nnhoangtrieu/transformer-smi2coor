{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted\n",
      "----------------------------------------\n",
      "Size: 4255\n",
      "Longest SMILES: 36\n",
      "Longest Coordinate: 22\n",
      "----------------------------------------\n",
      "smi_list (SMILES List):\n",
      "\t Cn1ncc(c1)B1OC(C(O1)(C)C)(C)CE\n",
      "smint_list (SMILES Integer List):\n",
      "\t [2, 3, 4, 3, 5, 5, 6, 5, 4, 7, 8, 4, 9, 2, 6, 2, 6, 9, 4, 7, 6, 2, 7, 2, 7, 6, 2, 7, 2, 1, 0, 0, 0, 0, 0, 0]\n",
      "coor_list (Coordinate List):\n",
      "\t [[4.8285, -1.004, 0.2024], [3.5776, -0.2572, 0.0479], [3.4435, 1.1346, 0.1047], [2.1893, 1.445, -0.0747], [1.4645, 0.2475, -0.2554], [2.3676, -0.7919, -0.1777], [-0.0805, 0.1225, -0.5047], [-1.0404, 1.1849, -0.5963], [-2.2048, 0.6858, 0.0949], [-2.0701, -0.8493, 0.0305], [-0.8409, -1.0789, -0.697], [-1.9777, -1.4359, 1.4405], [-3.2539, -1.4571, -0.7246], [-2.2055, 1.1603, 1.5494], [-3.4816, 1.1392, -0.6157]]\n",
      "np_coor_list (Normalized + Padded Coordinate List):\n",
      "\t tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-1.2500,  0.7500, -0.1500],\n",
      "        [-1.3900,  2.1400, -0.1000],\n",
      "        [-2.6400,  2.4500, -0.2800],\n",
      "        [-3.3600,  1.2500, -0.4600],\n",
      "        [-2.4600,  0.2100, -0.3800],\n",
      "        [-4.9100,  1.1300, -0.7100],\n",
      "        [-5.8700,  2.1900, -0.8000],\n",
      "        [-7.0300,  1.6900, -0.1100],\n",
      "        [-6.9000,  0.1500, -0.1700],\n",
      "        [-5.6700, -0.0700, -0.9000],\n",
      "        [-6.8100, -0.4300,  1.2400],\n",
      "        [-8.0800, -0.4500, -0.9300],\n",
      "        [-7.0300,  2.1600,  1.3500],\n",
      "        [-8.3100,  2.1400, -0.8200],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "from utils.data_preprocess import train_loader, test_loader, smi_list, smint_list, smi_dic, coor_list, np_coor_list, longest_coor, longest_smi, device\n",
    "from utils.helper import visualize, timeSince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head) :\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim_model // num_head\n",
    "\n",
    "        self.Q = nn.Linear(dim_model, dim_model)\n",
    "        self.K = nn.Linear(dim_model, dim_model)\n",
    "        self.V = nn.Linear(dim_model, dim_model)\n",
    "        print(self.dim_head)\n",
    "        self.out = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, Q, K, V) :\n",
    "        B = Q.size(0) # Shape Q, K, V: (B, longest_smi, dim_model)\n",
    "\n",
    "        Q, K, V = self.Q(Q), self.K(K), self.V(V)\n",
    "\n",
    "        len_Q, len_K, len_V = Q.size(1), K.size(1), V.size(1)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_head, len_Q, self.dim_head)\n",
    "        K = K.reshape(B, self.num_head, len_K, self.dim_head)\n",
    "        V = V.reshape(B, self.num_head, len_V, self.dim_head)\n",
    "        \n",
    "        K_T = K.transpose(2,3).contiguous()\n",
    "\n",
    "        attn_score = Q @ K_T\n",
    "\n",
    "        attn_score = attn_score / (self.dim_head ** 1/2)\n",
    "\n",
    "        attn_distribution = torch.softmax(attn_score, dim = -1)\n",
    "\n",
    "        attn = attn_distribution @ V\n",
    "\n",
    "        attn = attn.reshape(B, len_Q, self.num_head * self.dim_head)\n",
    "        \n",
    "        attn = self.out(attn)\n",
    "\n",
    "        return attn, attn_distribution\n",
    "\n",
    "class LSTM(nn.Module) :\n",
    "    def __init__(self, dim_model, longest_coor, num_head = 1, output_size = 3) :\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.longest_coor = longest_coor\n",
    "\n",
    "        self.cross_attn = SelfAttention(dim_model, num_head)\n",
    "\n",
    "        self.lstm = nn.LSTM(3 + dim_model, dim_model, batch_first=True)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        B = e_all.size(0)\n",
    "\n",
    "        d_input = torch.zeros(B, 1, 3).to(device)\n",
    "\n",
    "        d_hidden = e_last\n",
    "\n",
    "        d_outputs, cross_attn = [], []\n",
    "\n",
    "        for i in range(self.longest_coor) :\n",
    "            d_output, d_hidden, step_attn = self.forward_step(d_input, d_hidden, e_all)\n",
    "\n",
    "            d_outputs.append(d_output), cross_attn.append(step_attn)\n",
    "\n",
    "            if target is not None :\n",
    "                d_input = target[:, i, :].unsqueeze(1)\n",
    "            else :\n",
    "                d_input = d_output\n",
    "\n",
    "        d_outputs = torch.cat(d_outputs, dim = 1)\n",
    "\n",
    "        cross_attn = torch.cat(cross_attn, dim = 2)\n",
    "        \n",
    "        return d_outputs, d_hidden, cross_attn\n",
    "\n",
    "\n",
    "    def forward_step(self, d_input, d_hidden, e_all) :\n",
    "        Q = d_hidden.permute(1,0,2)\n",
    "\n",
    "        d_input = self.dropout(d_input)\n",
    "\n",
    "        attn, attn_distribution = self.cross_attn(Q, e_all, e_all)\n",
    "\n",
    "        input_lstm = torch.cat((attn, d_input), dim = 2)\n",
    "\n",
    "        output, _ = self.lstm(input_lstm) # Recheck about 2nd param\n",
    "\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, d_hidden, attn_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, fe, dropout) :\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.self_attn = SelfAttention(dim_model,num_head)\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.lstm = nn.LSTM(input_size=dim_model, hidden_size=dim_model, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_model, fe * dim_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fe * dim_model, dim_model)\n",
    "        )\n",
    "    def forward(self, Q, K, V) :\n",
    "\n",
    "        all_state, (last_state, _) = self.lstm(Q)\n",
    "\n",
    "        attn, attn_distribution = self.self_attn(all_state, all_state, all_state)\n",
    "\n",
    "        out = self.dropout(attn + all_state)\n",
    "\n",
    "        # out = self.dropout(self.norm1(attn + all_state))\n",
    "\n",
    "        # forward = self.feed_forward(x)\n",
    "\n",
    "        # out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "        return out, attn_distribution, last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, dim_model, num_block, num_head,\n",
    "                 len_dic, fe = 1, dropout = 0.1) :\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.embed = nn.Embedding(len_dic, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            EncoderBlock(dim_model, num_head, fe, dropout) for _ in range(num_block)\n",
    "        )\n",
    "\n",
    "    def forward(self, x) :\n",
    "        out = self.dropout(self.embed(x))\n",
    "\n",
    "        for block in self.encoder_blocks : \n",
    "            out, self_attn, last_state = block(out, out, out) \n",
    "        return out, last_state, self_attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, longest_coor, fe, dropout) :\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.lstm = LSTM(dim_model, longest_coor, num_head)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(3)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(3, fe * dim_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fe * dim_model, 3)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        output, _, cross_attn = self.lstm(e_all, e_last, target)\n",
    "        \n",
    "        # x = self.dropout(output)\n",
    "\n",
    "        # forward = self.feed_forward(x)\n",
    "\n",
    "        # out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "        return output, cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, dim_model,num_block, num_head, longest_coor, fe = 1, dropout = 0.1) :\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [DecoderBlock(dim_model, num_head,longest_coor, fe, dropout) for _ in range(num_block)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        for block in self.decoder_blocks :\n",
    "            target, cross_attn = block(e_all, e_last, target)\n",
    "        \n",
    "        return target, cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader,test_loader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, tf):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_test_loss = 0\n",
    "\n",
    "    for input, target in train_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad(), decoder_optimizer.zero_grad()\n",
    "        \n",
    "        e_all, e_last, self_attn = encoder(input)\n",
    "\n",
    "        # Teacher Forcing\n",
    "        if tf :\n",
    "          prediction, cross_attn = decoder(e_all, e_last, target)\n",
    "        else :\n",
    "          prediction, cross_attn = decoder(e_all, e_last)\n",
    "\n",
    "\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step(), decoder_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    encoder.eval(), decoder.eval()\n",
    "    \n",
    "\n",
    "\n",
    "    with torch.no_grad() :\n",
    "      for input, target in test_loader :\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        \n",
    "        e_all, e_last, self_attn = encoder(input)\n",
    "        prediction, cross_attn = decoder(e_all, e_last)\n",
    "\n",
    "        test_loss = criterion(prediction, target)\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader), total_test_loss / len(test_loader)\n",
    "\n",
    "\n",
    "def train(train_loader, test_loader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=1, visual_path= \"\", tf_rate = 1):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_total = 0  \n",
    "    test_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    tf = True\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      if epoch > (tf_rate * n_epochs) :\n",
    "        tf = False\n",
    "      encoder.train()\n",
    "      decoder.train()\n",
    "\n",
    "      train_loss, test_loss = train_epoch(train_loader, test_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, tf)\n",
    "      train_loss_total += train_loss\n",
    "      test_loss_total += test_loss\n",
    "\n",
    "      for i in range(5) :\n",
    "         r = random.randint(1, len(smi_list))\n",
    "         visualize(encoder, decoder, smi_list[r], smi_dic, longest_smi, mode=\"cross\", path=f\"{visual_path}\", name=f\"R{i}-CROSS-E{epoch}\")\n",
    "         visualize(encoder, decoder, smi_list[r], smi_dic, longest_smi, mode=\"self\", path=f\"{visual_path}\", name=f\"R{i}-SELF-E{epoch}\")\n",
    "\n",
    "      if epoch % print_every == 0:\n",
    "          train_loss_avg = train_loss_total / print_every\n",
    "          test_loss_avg = test_loss_total / print_every\n",
    "          train_loss_total = 0\n",
    "          test_loss_total = 0\n",
    "          print('%s (%d %d%%) /// Train loss: %.4f - Test loss: %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                      epoch, epoch / n_epochs * 100, train_loss_avg, test_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "DIM_MODEL = 256\n",
    "NUM_BLOCK = 1\n",
    "NUM_HEAD = 4\n",
    "DROPOUT = 0.1\n",
    "FE = 1\n",
    "\n",
    "\n",
    "encoder = Encoder(dim_model=DIM_MODEL,\n",
    "                  num_block=NUM_BLOCK,\n",
    "                  num_head=NUM_HEAD,\n",
    "                  dropout=DROPOUT,\n",
    "                  fe = FE,\n",
    "                  len_dic=len(smi_dic)).to(device)\n",
    "\n",
    "decoder = Decoder(dim_model=DIM_MODEL,\n",
    "                  num_block=NUM_BLOCK,\n",
    "                  num_head=NUM_HEAD,\n",
    "                  dropout=DROPOUT,\n",
    "                  fe=FE,\n",
    "                  longest_coor=longest_coor,\n",
    "                  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross attn: (8, 22, 36)\n",
      "self attn: (8, 36, 36)\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\OneDrive\\Desktop\\moffitt\\transformer-smi2coor\\utils\\helper.py:120: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + smi)\n",
      "c:\\Users\\DELL\\OneDrive\\Desktop\\moffitt\\transformer-smi2coor\\utils\\helper.py:129: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "visualize(encoder,\n",
    "          decoder,\n",
    "          smi=smi_list[12],\n",
    "          smi_dic=smi_dic,\n",
    "          longest_smi=longest_smi,\n",
    "          mode=\"cross\",\n",
    "          path=\"attention image\",\n",
    "          name=\"blala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
