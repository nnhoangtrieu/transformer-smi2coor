{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from train import train, train_epoch, train_loader, test_loader, smi_dic, longest_coor,longest_smi, device, timeSince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head) :\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_head = dim_model // num_head\n",
    "\n",
    "        self.Q = nn.Linear(dim_model, dim_model)\n",
    "        self.K = nn.Linear(dim_model, dim_model)\n",
    "        self.V = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, Q, K, V) :\n",
    "        B = Q.size(0) # Shape Q, K, V: (B, longest_smi, dim_model)\n",
    "\n",
    "        Q, K, V = self.Q(Q), self.K(K), self.V(V)\n",
    "\n",
    "        len_Q, len_K, len_V = Q.size(1), K.size(1), V.size(1)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_head, len_Q, self.dim_head)\n",
    "        K = K.reshape(B, self.num_head, len_K, self.dim_head)\n",
    "        V = V.reshape(B, self.num_head, len_V, self.dim_head)\n",
    "\n",
    "        K_T = K.transpose(2,3)\n",
    "\n",
    "        attn_score = Q @ K_T\n",
    "\n",
    "        attn_score = attn_score / (self.dim_head ** 1/2) \n",
    "\n",
    "        attn_distribution = torch.softmax(attn_score, dim = -1)\n",
    "\n",
    "        attn = attn_distribution @ V \n",
    "\n",
    "        attn = attn.reshape(B, len_Q, self.num_head * self.dim_head)\n",
    "\n",
    "        return attn, attn_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, fe, dropout) :\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.self_attn = SelfAttention(dim_model,num_head)\n",
    "        self.norm1 = nn.LayerNorm(dim_model) \n",
    "        self.norm2 = nn.LayerNorm(dim_model)\n",
    "        self.lstm = nn.LSTM(input_size=longest_smi, hidden_size=dim_model, batch_first=True)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_model, fe * dim_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fe * dim_model, dim_model)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q, K, V) :\n",
    "        attn, attn_distribution = self.self_attn(Q, K, V) \n",
    "\n",
    "        all_state, (last_state, _) = self.lstm(Q)\n",
    "\n",
    "        x = self.dropout(self.norm1(attn + all_state))\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "        return out, attn_distribution, last_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, dim_model, num_block, num_head,\n",
    "                 len_dic, fe = 1, dropout = 0.1) :\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "        self.embed = nn.Embedding(len_dic, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            EncoderBlock(dim_model, num_head, fe, dropout) for _ in range(num_block)\n",
    "        )\n",
    "\n",
    "    def forward(self, x) :\n",
    "        out = self.dropout(self.embed(x))\n",
    "\n",
    "        for block in self.encoder_blocks : \n",
    "            out, self_attn, last_state = block(out, out, out) \n",
    "        return out, last_state, self_attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module) :\n",
    "    def __init__(self, dim_model, longest_coor, num_head = 1, output_size = 3) :\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.longest_coor = longest_coor\n",
    "        self.cross_attn = SelfAttention(dim_model, num_head)\n",
    "        \n",
    "        self.lstm = nn.LSTM(3 + dim_model, dim_model, batch_first=True)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, output_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        B = e_all.size(0)\n",
    "\n",
    "        d_input = torch.zeros(B, 1, 3).to(device)\n",
    "\n",
    "        d_hidden = e_last \n",
    "\n",
    "        d_outputs, cross_attn = [], []\n",
    "\n",
    "        for i in range(self.longest_coor) :\n",
    "            d_output, d_hidden, step_attn = self.forward_step(d_input, d_hidden, e_all)\n",
    "\n",
    "            d_outputs.append(d_output), cross_attn.append(step_attn)\n",
    "\n",
    "            if target is not None :\n",
    "                d_input = target[:, i, :].unsqueeze(1)\n",
    "            else :\n",
    "                d_input = d_output\n",
    "        \n",
    "        d_outputs = torch.cat(d_outputs, dim = 1)\n",
    "        cross_attn = torch.cat(cross_attn, dim = 1)\n",
    "\n",
    "        return d_outputs, d_hidden, cross_attn\n",
    "    \n",
    "\n",
    "    def forward_step(self, d_input, d_hidden, e_all) :\n",
    "        Q = d_hidden.permute(1,0,2)\n",
    "\n",
    "        d_input = self.dropout(d_input)\n",
    "\n",
    "        attn, attn_distribution = self.cross_attn(Q, e_all, e_all)\n",
    "\n",
    "        input_lstm = torch.cat((attn, d_input), dim = 2)\n",
    "\n",
    "        output, _ = self.lstm(input_lstm) # Recheck about 2nd param\n",
    "\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, d_hidden, attn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module) :\n",
    "    def __init__(self, dim_model, num_head, longest_coor, fe, dropout) :\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.lstm = LSTM(dim_model, longest_coor, num_head)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_model)\n",
    "        self.norm2 = nn.LayerNorm(3)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(3, fe * dim_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fe * dim_model, 3)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        output, _, cross_attn = self.lstm(e_all, e_last, target)\n",
    "        x = self.dropout(output)\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "        return out, cross_attn\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, dim_model,num_block, num_head, longest_coor, fe = 1, dropout = 0.1) :\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [DecoderBlock(dim_model, num_head,longest_coor, fe, dropout) for _ in range(num_block)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, e_all, e_last, target = None) :\n",
    "        for block in self.decoder_blocks :\n",
    "            target, cross_attn = block(e_all, e_last, target)\n",
    "        \n",
    "        return target, cross_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = 0 \n",
    "\n",
    "for input, target in train_loader :\n",
    "    test_input = input \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 256\n",
    "encoder = Encoder(dim_model=512,\n",
    "                  num_block=2,\n",
    "                  num_head=2,\n",
    "                  len_dic=len(smi_dic))\n",
    "\n",
    "decoder = Decoder(dim_model=512,\n",
    "                  num_block=1,\n",
    "                  num_head=1,\n",
    "                  longest_coor=longest_coor,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 22, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\OneDrive\\Desktop\\moffitt\\transformer-smi2coor\\data_preprocess.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.x[idx], dtype = torch.long, device=device), torch.tensor(self.y[idx], device = device)\n"
     ]
    }
   ],
   "source": [
    "for input, target in train_loader :\n",
    "    e_all, e_last, self_attn = encoder(input)\n",
    "    prediction, cross_attn = decoder(e_all, e_last)\n",
    "    print(prediction.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader,test_loader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, tf):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_test_loss = 0\n",
    "\n",
    "    for input, target in train_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad(), decoder_optimizer.zero_grad()\n",
    "        \n",
    "        e_all, e_last, self_attn = encoder(input)\n",
    "\n",
    "        # Teacher Forcing\n",
    "        if tf :\n",
    "          prediction, cross_attn = decoder(e_all, e_last, target)\n",
    "        else :\n",
    "          prediction, cross_attn = decoder(e_all, e_last)\n",
    "\n",
    "\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step(), decoder_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    encoder.eval(), decoder.eval()\n",
    "    \n",
    "\n",
    "\n",
    "    with torch.no_grad() :\n",
    "      for input, target in test_loader :\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        \n",
    "        e_all, e_last, self_attn = encoder(input)\n",
    "        prediction, cross_attn = decoder(e_all, e_last)\n",
    "\n",
    "        test_loss = criterion(prediction, target)\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader), total_test_loss / len(test_loader)\n",
    "\n",
    "\n",
    "def train(train_loader, test_loader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=1, visual_path= \"\", tf_rate = 1):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss_total = 0  \n",
    "    test_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    tf = True\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      if epoch > (tf_rate * n_epochs) :\n",
    "        tf = False\n",
    "      encoder.train()\n",
    "      decoder.train()\n",
    "\n",
    "      train_loss, test_loss = train_epoch(train_loader, test_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, tf)\n",
    "      train_loss_total += train_loss\n",
    "      test_loss_total += test_loss\n",
    "\n",
    "    #   for i in range(5) :\n",
    "    #      visualize(encoder, decoder, smi_list[i], smi_dic, longest_smi, mode=\"cross\", path=f\"{visual_path}\", name=f\"{i}-cross-E{epoch}\")\n",
    "    #      visualize(encoder, decoder, smi_list[i], smi_dic, longest_smi, mode=\"self\", path=f\"{visual_path}\", name=f\"{i}-self-E{epoch}\")\n",
    "\n",
    "      if epoch % print_every == 0:\n",
    "          train_loss_avg = train_loss_total / print_every\n",
    "          test_loss_avg = test_loss_total / print_every\n",
    "          train_loss_total = 0\n",
    "          test_loss_total = 0\n",
    "          print('%s (%d %d%%) /// Train loss: %.4f - Test loss: %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                      epoch, epoch / n_epochs * 100, train_loss_avg, test_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
